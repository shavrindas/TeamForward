import os
from data_handler.data_converter import resize_images

def main():
    # 현재 스크립트 파일의 디렉토리 경로
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # 상위 디렉토리로 이동
    parent_dir = os.path.dirname(script_dir)

    # 작업할 폴더의 상대 경로 설정
    input_folder = os.path.join(parent_dir, 'resource', 'temp_pic')
    output_folder = os.path.join(parent_dir, 'resource', 'set_pic')

    # 입력 폴더가 존재하고, 파일이 있는 경우
    if os.path.exists(input_folder) and os.listdir(input_folder):
        resize_images(input_folder, output_folder)

if __name__ == "__main__":
    main()
    



def calculate_hash(image_path):
    with open(image_path, 'rb') as f:
        image_data = f.read()
        hash_value = hashlib.md5(image_data).hexdigest()
    return hash_value

def download_image(url, folder_path):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            current_time = datetime.now().strftime("%Y%m%d%H%M%S")
            file_name = f"{current_time}_{os.path.basename(urlparse(url).path)}"
            file_path = os.path.join(folder_path, file_name)
            with open(file_path, 'wb') as f:
                f.write(response.content)
            print(f"이미지 다운로드 완료: {url}")
            return file_path
        else:
            print(f"이미지 다운로드 실패: {url}")
    except Exception as e:
        print(f"오류 발생: {e}")

def get_image_urls(search_query, num_images):
    search_url = f"https://www.google.com/search?q={search_query}&tbm=isch"
    response = requests.get(search_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    image_urls = []
    for img in soup.find_all('img', limit=num_images):
        img_url = img.get('src')
        if img_url:
            image_urls.append(img_url)
    return image_urls

def remove_duplicate_images(image_paths):
    unique_image_paths = []
    unique_image_hashes = set()
    for image_path in image_paths:
        image_hash = calculate_hash(image_path)
        if image_hash not in unique_image_hashes:
            unique_image_hashes.add(image_hash)
            unique_image_paths.append(image_path)
        else:
            os.remove(image_path)
            print(f"중복된 이미지 제거: {image_path}")
    return unique_image_paths
