import os
from data_handler.data_converter import resize_images

def main():
    # 현재 스크립트 파일의 디렉토리 경로
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # 상위 디렉토리로 이동
    parent_dir = os.path.dirname(script_dir)

    # 작업할 폴더의 상대 경로 설정
    input_folder = os.path.join(parent_dir, 'resource', 'temp_pic')
    output_folder = os.path.join(parent_dir, 'resource', 'set_pic')

    # 입력 폴더가 존재하고, 파일이 있는 경우
    if os.path.exists(input_folder) and os.listdir(input_folder):
        resize_images(input_folder, output_folder)

if __name__ == "__main__":
    main()
    



def calculate_hash(image_path):
    with open(image_path, 'rb') as f:
        image_data = f.read()
        hash_value = hashlib.md5(image_data).hexdigest()
    return hash_value

def download_image(url, folder_path):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            current_time = datetime.now().strftime("%Y%m%d%H%M%S")
            file_name = f"{current_time}_{os.path.basename(urlparse(url).path)}"
            file_path = os.path.join(folder_path, file_name)
            with open(file_path, 'wb') as f:
                f.write(response.content)
            print(f"이미지 다운로드 완료: {url}")
            return file_path
        else:
            print(f"이미지 다운로드 실패: {url}")
    except Exception as e:
        print(f"오류 발생: {e}")

def get_image_urls(search_query, num_images):
    search_url = f"https://www.google.com/search?q={search_query}&tbm=isch"
    response = requests.get(search_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    image_urls = []
    for img in soup.find_all('img', limit=num_images):
        img_url = img.get('src')
        if img_url:
            image_urls.append(img_url)
    return image_urls

def remove_duplicate_images(image_paths):
    unique_image_paths = []
    unique_image_hashes = set()
    for image_path in image_paths:
        image_hash = calculate_hash(image_path)
        if image_hash not in unique_image_hashes:
            unique_image_hashes.add(image_hash)
            unique_image_paths.append(image_path)
        else:
            os.remove(image_path)
            print(f"중복된 이미지 제거: {image_path}")
    return unique_image_paths
///



import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# GPU 디바이스 사용 설정
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

# 데이터 경로 설정
train_data_dir = "E:\\Data\\008.의류 통합 데이터(착용 이미지, 치수 및 원단 정보)\\01-1.정식개방데이터\\Validation\\01.원천데이터"
validation_data_dir = "D:\\unsafe_code\\TeamForward\\back\\Dina\\resource\\data_get_pic"
img_width, img_height = 224, 224
batch_size = 32

# 이미지 데이터 생성기 설정
train_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = validation_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

# 모델 구현
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(12, activation='softmax')
])

# 모델 컴파일
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# 학습하지 않고 이미 학습된 모델을 로드할 경우 아래 주석 처리된 부분을 사용합니다.
model = tf.keras.models.load_model("clothing_classification_model.h5")

# 모델 평가
loss, accuracy = model.evaluate(validation_generator)
print("Validation Loss:", loss)
print("Validation Accuracy:", accuracy)
